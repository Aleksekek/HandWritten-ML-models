{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=14, n_informative=10, random_state=42)\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.Series(y)\n",
    "X.columns = [f'col_{col}' for col in X.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_0</th>\n",
       "      <th>col_1</th>\n",
       "      <th>col_2</th>\n",
       "      <th>col_3</th>\n",
       "      <th>col_4</th>\n",
       "      <th>col_5</th>\n",
       "      <th>col_6</th>\n",
       "      <th>col_7</th>\n",
       "      <th>col_8</th>\n",
       "      <th>col_9</th>\n",
       "      <th>col_10</th>\n",
       "      <th>col_11</th>\n",
       "      <th>col_12</th>\n",
       "      <th>col_13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.212170</td>\n",
       "      <td>0.588157</td>\n",
       "      <td>0.373931</td>\n",
       "      <td>-1.213137</td>\n",
       "      <td>-0.431857</td>\n",
       "      <td>-1.805413</td>\n",
       "      <td>0.374652</td>\n",
       "      <td>-0.328778</td>\n",
       "      <td>1.662872</td>\n",
       "      <td>0.682909</td>\n",
       "      <td>-0.635783</td>\n",
       "      <td>-0.096290</td>\n",
       "      <td>-0.706476</td>\n",
       "      <td>1.475155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.136309</td>\n",
       "      <td>-0.340340</td>\n",
       "      <td>-1.518135</td>\n",
       "      <td>2.791709</td>\n",
       "      <td>-0.348785</td>\n",
       "      <td>-0.697299</td>\n",
       "      <td>-3.616860</td>\n",
       "      <td>-0.644757</td>\n",
       "      <td>-3.150791</td>\n",
       "      <td>0.153453</td>\n",
       "      <td>-1.975852</td>\n",
       "      <td>1.927038</td>\n",
       "      <td>-0.225723</td>\n",
       "      <td>1.335919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.995246</td>\n",
       "      <td>0.418912</td>\n",
       "      <td>-1.147293</td>\n",
       "      <td>4.705204</td>\n",
       "      <td>0.109306</td>\n",
       "      <td>-0.134241</td>\n",
       "      <td>0.297226</td>\n",
       "      <td>2.162918</td>\n",
       "      <td>-6.801806</td>\n",
       "      <td>1.573732</td>\n",
       "      <td>0.234367</td>\n",
       "      <td>-0.348181</td>\n",
       "      <td>-3.033989</td>\n",
       "      <td>-2.326364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4.434309</td>\n",
       "      <td>1.959684</td>\n",
       "      <td>0.313601</td>\n",
       "      <td>0.497666</td>\n",
       "      <td>0.864826</td>\n",
       "      <td>2.565846</td>\n",
       "      <td>-1.654235</td>\n",
       "      <td>-1.603219</td>\n",
       "      <td>1.411960</td>\n",
       "      <td>-0.621943</td>\n",
       "      <td>-2.532930</td>\n",
       "      <td>-0.387911</td>\n",
       "      <td>0.313242</td>\n",
       "      <td>4.148565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-6.564804</td>\n",
       "      <td>0.302972</td>\n",
       "      <td>0.394640</td>\n",
       "      <td>1.189341</td>\n",
       "      <td>-2.472628</td>\n",
       "      <td>4.819816</td>\n",
       "      <td>-1.126806</td>\n",
       "      <td>-1.453735</td>\n",
       "      <td>-2.691496</td>\n",
       "      <td>-0.259630</td>\n",
       "      <td>-1.587911</td>\n",
       "      <td>-0.205920</td>\n",
       "      <td>-0.589160</td>\n",
       "      <td>0.759591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-0.526731</td>\n",
       "      <td>-1.008107</td>\n",
       "      <td>-4.400507</td>\n",
       "      <td>4.747371</td>\n",
       "      <td>-0.875473</td>\n",
       "      <td>-3.828352</td>\n",
       "      <td>-2.089767</td>\n",
       "      <td>-2.770453</td>\n",
       "      <td>-3.079070</td>\n",
       "      <td>1.040369</td>\n",
       "      <td>-0.038261</td>\n",
       "      <td>3.565952</td>\n",
       "      <td>0.084973</td>\n",
       "      <td>1.630007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-2.103575</td>\n",
       "      <td>0.431284</td>\n",
       "      <td>-0.754849</td>\n",
       "      <td>1.570126</td>\n",
       "      <td>-0.625290</td>\n",
       "      <td>2.505303</td>\n",
       "      <td>1.858279</td>\n",
       "      <td>-0.591713</td>\n",
       "      <td>2.611486</td>\n",
       "      <td>-2.494160</td>\n",
       "      <td>-0.771243</td>\n",
       "      <td>1.455433</td>\n",
       "      <td>-1.174142</td>\n",
       "      <td>0.142873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>-7.547149</td>\n",
       "      <td>-0.604030</td>\n",
       "      <td>-3.230908</td>\n",
       "      <td>1.485347</td>\n",
       "      <td>-1.663776</td>\n",
       "      <td>1.225091</td>\n",
       "      <td>-2.065999</td>\n",
       "      <td>2.810627</td>\n",
       "      <td>-4.968167</td>\n",
       "      <td>-0.477255</td>\n",
       "      <td>-3.990299</td>\n",
       "      <td>-1.503262</td>\n",
       "      <td>0.219204</td>\n",
       "      <td>-1.434965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-4.256086</td>\n",
       "      <td>-1.225535</td>\n",
       "      <td>-2.306923</td>\n",
       "      <td>0.987779</td>\n",
       "      <td>1.735099</td>\n",
       "      <td>3.331157</td>\n",
       "      <td>2.363935</td>\n",
       "      <td>1.343506</td>\n",
       "      <td>1.423395</td>\n",
       "      <td>0.452345</td>\n",
       "      <td>-2.211194</td>\n",
       "      <td>0.828361</td>\n",
       "      <td>0.134801</td>\n",
       "      <td>-1.086124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-0.306125</td>\n",
       "      <td>0.773548</td>\n",
       "      <td>0.510712</td>\n",
       "      <td>-1.957401</td>\n",
       "      <td>-2.299229</td>\n",
       "      <td>-2.031844</td>\n",
       "      <td>-2.030080</td>\n",
       "      <td>1.033079</td>\n",
       "      <td>-0.020384</td>\n",
       "      <td>-0.982392</td>\n",
       "      <td>-1.586209</td>\n",
       "      <td>-0.864263</td>\n",
       "      <td>-1.050941</td>\n",
       "      <td>0.635891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        col_0     col_1     col_2     col_3     col_4     col_5     col_6  \\\n",
       "0    0.212170  0.588157  0.373931 -1.213137 -0.431857 -1.805413  0.374652   \n",
       "1   -2.136309 -0.340340 -1.518135  2.791709 -0.348785 -0.697299 -3.616860   \n",
       "2   -2.995246  0.418912 -1.147293  4.705204  0.109306 -0.134241  0.297226   \n",
       "3   -4.434309  1.959684  0.313601  0.497666  0.864826  2.565846 -1.654235   \n",
       "4   -6.564804  0.302972  0.394640  1.189341 -2.472628  4.819816 -1.126806   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "995 -0.526731 -1.008107 -4.400507  4.747371 -0.875473 -3.828352 -2.089767   \n",
       "996 -2.103575  0.431284 -0.754849  1.570126 -0.625290  2.505303  1.858279   \n",
       "997 -7.547149 -0.604030 -3.230908  1.485347 -1.663776  1.225091 -2.065999   \n",
       "998 -4.256086 -1.225535 -2.306923  0.987779  1.735099  3.331157  2.363935   \n",
       "999 -0.306125  0.773548  0.510712 -1.957401 -2.299229 -2.031844 -2.030080   \n",
       "\n",
       "        col_7     col_8     col_9    col_10    col_11    col_12    col_13  \n",
       "0   -0.328778  1.662872  0.682909 -0.635783 -0.096290 -0.706476  1.475155  \n",
       "1   -0.644757 -3.150791  0.153453 -1.975852  1.927038 -0.225723  1.335919  \n",
       "2    2.162918 -6.801806  1.573732  0.234367 -0.348181 -3.033989 -2.326364  \n",
       "3   -1.603219  1.411960 -0.621943 -2.532930 -0.387911  0.313242  4.148565  \n",
       "4   -1.453735 -2.691496 -0.259630 -1.587911 -0.205920 -0.589160  0.759591  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "995 -2.770453 -3.079070  1.040369 -0.038261  3.565952  0.084973  1.630007  \n",
       "996 -0.591713  2.611486 -2.494160 -0.771243  1.455433 -1.174142  0.142873  \n",
       "997  2.810627 -4.968167 -0.477255 -3.990299 -1.503262  0.219204 -1.434965  \n",
       "998  1.343506  1.423395  0.452345 -2.211194  0.828361  0.134801 -1.086124  \n",
       "999  1.033079 -0.020384 -0.982392 -1.586209 -0.864263 -1.050941  0.635891  \n",
       "\n",
       "[1000 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "995    0\n",
       "996    1\n",
       "997    1\n",
       "998    0\n",
       "999    0\n",
       "Length: 1000, dtype: int32"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogReg():\n",
    "    def __init__(self, n_iter: int = 10, learning_rate: float = 0.1):\n",
    "        self.n_iter = n_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = None\n",
    "\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        params = [f'{key}={value}' for key, value in self.__dict__.items() if value]\n",
    "        return 'MyLogReg class: ' + ', '.join(params)\n",
    "    \n",
    "\n",
    "    def fit(self, X:pd.DataFrame, y:pd.Series, verbose=False):\n",
    "        X['ones_col'] = 1\n",
    "        X=X[['ones_col'] + [col for col in X.columns[:-1]]]\n",
    "        self.weights = [1 for feature in X.columns]\n",
    "        for iter in range(1, self.n_iter+1):\n",
    "            y_pred = 1/(1+np.exp(-np.dot(X, self.weights)))\n",
    "            eps = 1e-15\n",
    "            f_sum = y*np.log(y_pred + eps)\n",
    "            s_sum = (1-y)*np.log(1-y_pred+eps)\n",
    "            logloss = -sum(f_sum+s_sum)/len(y)\n",
    "            gradient = np.dot((y_pred - y), X)/len(y)\n",
    "            self.weights = self.weights - self.learning_rate*gradient\n",
    "            if verbose:\n",
    "                if verbose and (iter == 1):\n",
    "                    print(f'start | loss: {logloss}')\n",
    "                elif iter%verbose == 0:\n",
    "                    print(f'{iter} | loss: {logloss}')\n",
    "\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X['ones_col'] = 1\n",
    "        X=X[['ones_col'] + [col for col in X.columns[:-1]]]\n",
    "        y_pred = 1/(1+np.exp(-np.dot(X, self.weights)))\n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "    def predict(self, X):\n",
    "        pred = list(self.predict_proba(X))\n",
    "        return pd.Series([1 if i>0.5 else 0 for i in pred])\n",
    "\n",
    "\n",
    "    def get_coef(self):\n",
    "        return self.weights[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyLogReg class: n_iter=50, learning_rate=0.1\n"
     ]
    }
   ],
   "source": [
    "mlr = MyLogReg(50)\n",
    "print(mlr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start | loss: 3.6736886876615538\n",
      "10 | loss: 1.8729518266222953\n",
      "20 | loss: 1.1626945313705068\n",
      "30 | loss: 0.8288847284427477\n",
      "40 | loss: 0.654884590222696\n",
      "50 | loss: 0.5606818660628202\n"
     ]
    }
   ],
   "source": [
    "mlr.fit(X, y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "611"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlr.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3330418371591503"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(mlr.get_coef())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05247374,  0.63579113,  0.35707211,  0.20338172, -0.71400341,\n",
       "        0.65306157, -0.30353191,  0.74286901,  0.14726892,  0.16403292,\n",
       "        0.81238906,  0.0167088 ,  0.67663333,  1.21843872])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlr.get_coef()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
